name: baseline+smart_init
model: facebook/bart-large

penman_linearization: True
use_pointer_tokens: True
raw_graph: False

remove_wiki: False
dereify: False
collapse_name_ops: False

# Hparams
batch_size: 500
beam_size: 1
dropout: 0.25
attention_dropout: 0.0
smart_init: True
accum_steps: 20
warmup_steps: 1000
training_steps: 20000
weight_decay: 0.004
grad_norm: 2.5
scheduler: cosine
learning_rate: 0.00003
max_epochs: 40
save_checkpoints: True
log_wandb: False
warm_start: True
use_recategorization: False
best_loss: False
remove_longer_than: 512

# <------------------
# Data: replace DATA below with the root of your AMR 2/3 release folder

# Example for AMR 2 without recategorization
# train: data/LDC2017T10/data/amrs/split/training/*.txt
# dev: data/LDC2017T10/data/amrs/split/dev/*.txt
# test: data/LDC2017T10/data/amrs/split/test/*.txt

# Example for AMR 2 with recategorization
train: data/amr_2_reca/train.txt.features.preproc
dev: data/amr_2_reca/dev.txt.features.preproc
test: data/amr_2_reca/test.txt.features.preproc

# Example for AMR 3 without recategorization
# train: data/amr_annotation_3.0/data/amrs/split/training/*.txt
# dev: data/amr_annotation_3.0/data/amrs/split/dev/*.txt
# test: data/amr_annotation_3.0/data/amrs/split/test/*.txt
