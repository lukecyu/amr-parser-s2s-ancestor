name: baseline+smart_init
model: facebook/bart-large

# <--------------
# Linearizations
# Comment DFS and uncomment the relevant block if you want to use a different linearization scheme

# DFS
penman_linearization: True
use_pointer_tokens: True
raw_graph: False

# BFS
# penman_linearization: False
# use_pointer_tokens: True
# raw_graph: False

# PENMAN
# penman_linearization: True
# use_pointer_tokens: False
# raw_graph: False

# BART baseline
# penman_linearization: True
# use_pointer_tokens: False
# raw_graph: True

remove_wiki: False
dereify: False
collapse_name_ops: False

# parent_embedding
parent_embedding: True

# Hparams
batch_size: 500
beam_size: 1
dropout: 0.25
attention_dropout: 0.0
smart_init: True
accum_steps: 20
warmup_steps: 1
training_steps: 250000
weight_decay: 0.004
grad_norm: 2.5
scheduler: constant
learning_rate: 0.00003
max_epochs: 40
save_checkpoints: True
log_wandb: False
warm_start: True
use_recategorization: False
best_loss: False
remove_longer_than: 1024

# <------------------
# Data: replace DATA below with the root of your AMR 2/3 release folder
#train: ../amr_data/LDC2017T10/data/amrs/split/training/*.txt
#dev: ../amr_data/LDC2017T10/data/amrs/split/dev/*.txt
#test: ../amr_data/LDC2017T10/data/amrs/split/test/*.txt
#train: data/amr_2_reca/train.txt.features.preproc
#dev: data/amr_2_reca/dev.txt.features.preproc
#test: data/amr_2_reca/test.txt.features.preproc
train: ../amr_data/amr_annotation_3.0/data/amrs/split/training/*.txt
dev: ../amr_data/amr_annotation_3.0/data/amrs/split/dev/*.txt
test: ../amr_data/amr_annotation_3.0/data/amrs/split/test/*.txt
